# 	RL in continuous Spac

### recap:



![1558008613822](/home/roees/DRL course/typoraImages/Part1/ContRL_intro.png)



In this module we will learn how to generalize RL of discrete state-action values to continuous one:

* Deep Q-Learning
* policy gradients
  * actor-critic methods (mix the best of both worlds)

### Discrete vs. Continuous Spaces







![1558010531433](/home/roees/DRL course/typoraImages/Part1/ContRL_intro_2.png)

![1558010647649](/home/roees/DRL course/typoraImages/Part1/ContRL_intro_3.png)



Two main stratagies for coping with continuous space that we will be looking at are:

![1558010882917](/home/roees/DRL course/typoraImages/Part1/ContRL_intro_4.png)





### Discretization

Enables us to use existing algorithms with little or no modifications.

![1558011115231](/home/roees/DRL course/typoraImages/Part1/ContRL_intro_5.png)



![1558011220708](/home/roees/DRL course/typoraImages/Part1/ContRL_intro_6.png)



#### Workshop Discretization

the files are in git...

