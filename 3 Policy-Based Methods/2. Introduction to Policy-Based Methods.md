# 2. Introduction to Policy-Based Methods

In this lesson, you'll learn all about **policy-based methods**. Watch the video below to learn what policy-based methods are, and how they differ from value-based methods!

example of value based method (cart-pole):

![1560435452112](C:\Users\ROEE\GitProjects\DRLL\3 Policy-Based Methods\images\introToPbm.png)

From the above trained network we could obtain a value for each action at a given state and then choose a policy (e-greedy).

In policy-based methods the following network is used:

![1560435684321](C:\Users\ROEE\AppData\Roaming\Typora\typora-user-images\1560435684321.png)

The output is probability to sample from when choosing an action.

**<u>Note:</u>** A valid activation function for the output layer is softmax.

softmax =

<img src='images\softmax.png' style='zoom:40%'>

In this lesson we will learn many different ways to optimize these weights.



## What about continuous action spaces?

The CartPole environment has a discrete action space. So, how do we use a neural network to approximate a policy, if the environment has a continuous action space?

As you learned above, in the case of **discrete** action spaces, the neural network has one node for each possible action.

For **continuous** action spaces, the neural network has one node for each action entry (or index). For example, consider the action space of the [bipedal walker](https://github.com/openai/gym/wiki/BipedalWalker-v2) environment, shown in the figure below.

<img src='images/screen-shot-2018-07-01-at-11.28.57-am.png' style='zoom:40%'>

In this case, any action is a vector of four numbers, so the output layer of the policy network will have four nodes.

Since every entry in the action must be a number between -1 and 1, we will add a [tanh activation function](https://pytorch.org/docs/stable/nn.html#torch.nn.Tanh) to the output layer.

As another example, consider the [continuous mountain car](https://github.com/openai/gym/wiki/MountainCarContinuous-v0) benchmark. The action space is shown in the figure below. Note that for this environment, the action must be a value between -1 and 1.

<img src='images\screen-shot-2018-07-01-at-11.19.22-am.png' style='zoom:40%'>

An appropriate output layer for the policy would be a layer of size: 1 and  tanh as the activation function.



## Maximizing Expected Return (denoted as $J$) in PBM

<img src='images\maximizingExpectedReturn.png' style='zoom:60%'>



Hill climbing is using gradient ascent to find $\theta$.

**<u>Local Minima</u>**

Gradient ascent is a relatively simple algorithm that the agent can use to gradually improve the weights $\theta$ in its policy network while interacting with the environment.

Note, however, that it's **not** guaranteed to always yield the weights of the optimal policy. This is because we can easily get stuck in a local maximum. In this lesson, you'll learn about some policy-based methods that are less prone to this.

### Hill Climbing Pseudocode

<img src='images\hillClimbing.png' style='zoom:60%'>



### What's the difference between G and J?


You might be wondering: what's the difference between the return that the agent collects in a single episode (G, *from the pseudocode above*) and the expected return J?

Well ... in reinforcement learning, the goal of the agent is to find the value of the policy network weights \theta*θ* that maximizes **expected** return, which we have denoted by J.

In the hill climbing algorithm, the values of \theta*θ* are evaluated according to how much return G*G* they collected in a **single episode**. To see that this might be a little bit strange, note that due to randomness in the environment (and the policy, if it is stochastic), it is highly likely that if we collect a second episode with the same values for $\theta$, we'll likely get a different value for the return *G*. Because of this, the (sampled) return G is not a perfect estimate for the expected return J, but it often turns out to be **good enough** in practice.

<u>*Note*</u>: We refer to the general class of approaches that find $\arg\max_{\theta}J(\theta)$ through randomly perturbing the most recent best estimate as **stochastic policy search**. Likewise, we can refer to *J* as an **objective function**, which just refers to the fact that we'd like to *maximize* it!

Simulated Annealing: is reducing the additive gaussian noise as we get better and better policies. If no better policies are found then increasing the scale of the gaussian noise should land a policy out of the local minima. This is called also **adaptive noise scaling**.



# More Black-Box Optimization

All of the algorithms that you’ve learned about in this lesson can be classified as **black-box optimization** techniques.

**Black-box** refers to the fact that in order to find the value of \theta*θ* that maximizes the function $J = J(\theta)$, we need only be able to estimate the value of *J* at any potential value of $\theta$.

That is, both hill climbing and steepest ascent hill climbing don't know that we're solving a reinforcement learning problem, and they do not care that the function we're trying to maximize corresponds to the expected return.

These algorithms only know that for each value of $\theta$, there's a corresponding **number**. We know that this **number** corresponds to the return obtained by using the policy corresponding to $\theta$ to collect an episode, but the algorithms are not aware of this. To the algorithms, the way we evaluate $\theta$ is considered a black box, and they don't worry about the details. The algorithms only care about finding the value of $\theta$ that will maximize the number that comes out of the black box.