# Proximal Policy Optimization


## Lesson Preview

State-of-the-art RL algorithms contain many important tweaks in addition to simple value-based or policy-based methods. One of these key improvements is called Proximal Policy Optimization (PPO) -- also closely related to Trust Region Policy Optimization (TRPO). It has allowed faster and more stable learning. From developing agile robots, to creating expert level gaming AI, PPO has proven useful in a wide domain of applications, and has become part of the standard toolkits in complicated learning environments.

In this lesson, we will first review the most basic policy gradient algorithm -- REINFORCE, and discuss issues associated with the algorithm. We will get an in-depth understanding of why these problems arise, and find ways to fix them. The solutions will lead us to PPO. Our lesson will focus on learning the intuitions behind why and how PPO improves learning, and implement it to teach a computer to play Atari-Pong, using only the pixels as input.

*The idea of PPO was published by the team at OpenAI, and you can read their paper through this [link](https://arxiv.org/abs/1707.06347)*



## Beyond REINFORCE

Here, we briefly review key ingredients of the REINFORCE algorithm.

REINFORCE works as follows: First, we initialize a random policy $\pi_\theta(a;s)$, and using the policy we collect a trajectory -- or a list of (state, actions, rewards) at each time step:
$$
s_1, a_1, r_1, s_2, a_2, r_2, ...
$$
Second, we compute the total reward of the trajectory $R=r_1+r_2+r_3+...$, and compute an estimate of the gradient of the expected reward, *g*:
$$
g = R \sum_t \nabla_\theta \log\pi_\theta(a_t|s_t)
$$
Third, we update our policy using gradient ascent with learning rate $\alpha$:
$$
\theta \leftarrow \theta + \alpha g
$$
The process then repeats.



What are the main problems of REINFORCE? There are three issues:

1. The update process is very **inefficient**! We run the policy once, update once, and then throw away the trajectory.
2. The gradient estimate *g* is very **noisy**. By chance the collected trajectory may not be representative of the policy.
3. There is no clear **credit assignment**. A trajectory may contain many good/bad actions and whether these actions are reinforced depends only on the final total output.

In the following concepts, we will go over ways to improve the REINFORCE algorithm and resolve all 3 issues. All of the improvements will be utilized and implemented in the PPO algorithm.



## Noise Reduction

The way we optimize the policy is by maximizing the average rewards $U(\theta)$. To do that we use stochastic gradient ascent. Mathematically, the gradient is given by an average over all the possible trajectories,
$$
\nabla_\theta U(\theta) = \overbrace{\sum_\tau P(\tau; \theta)}^{ \begin{matrix} \scriptsize\textrm{average over}\\ \scriptsize\textrm{all trajectories} \end{matrix} } \underbrace{\left( R_\tau \sum_t \nabla_\theta \log \pi_\theta(a_t^{(\tau)}|s_t^{(\tau)}) \right)}_{ \textrm{only one is sampled} }
$$
There could easily be well over millions of trajectories for simple problems, and infinite for continuous problems.

For practical purposes, we simply take one trajectory to compute the gradient, and update our policy. So a lot of times, the result of a sampled trajectory comes down to chance, and doesn't contain that much information about our policy. How does learning happen then? The hope is that after training for a long time, the tiny signal accumulates.

The easiest option to reduce the noise in the gradient is to simply sample more trajectories! Using distributed computing, we can collect multiple trajectories in parallel, so that it won’t take too much time. Then we can estimate the policy gradient by averaging across all the different trajectories
$$
\left. \begin{matrix} s^{(1)}_t, a^{(1)}_t, r^{(1)}_t\\[6pt] s^{(2)}_t, a^{(2)}_t, r^{(2)}_t\\[6pt] s^{(3)}_t, a^{(3)}_t, r^{(3)}_t\\[6pt] \vdots \end{matrix} \;\; \right\}\!\!\!\! \rightarrow g = \frac{1}{N}\sum_{i=1}^N R_i \sum_t\nabla_\theta \log \pi_\theta(a^{(i)}_t | s^{(i)}_t)
$$

## Rewards Normalization

There is another bonus for running multiple trajectories: we can collect all the total rewards and get a sense of how they are distributed.

In many cases, the distribution of rewards shifts as learning happens. Reward = 1 might be really good in the beginning, but really bad after 1000 training episode.

Learning can be improved if we normalize the rewards, where \mu*μ* is the mean, and \sigma*σ* the standard deviation.
$$
R_i \leftarrow \frac{R_i -\mu}{\sigma} \qquad \mu = \frac{1}{N}\sum_i^N R_i \qquad \sigma = \sqrt{\frac{1}{N}\sum_i (R_i - \mu)^2}
$$

(when all the $R_i$ are the same, $\sigma =0$, we can set all the normalized rewards to 0 to avoid numerical problems)

This batch-normalization technique is also used in many other problems in AI (e.g. image classification), where normalizing the input can improve learning.

Intuitively, normalizing the rewards roughly corresponds to picking half the actions to encourage/discourage, while also making sure the steps for gradient ascents are not too large/small.



## Credit Assignment

Going back to the gradient estimate, we can take a closer look at the total reward *R*, which is just a sum of reward at each step $R=r_1+r_2+...+r_{t-1}+r_t+...$
$$
g=\sum_t (...+r_{t-1}+r_{t}+...)\nabla_{\theta}\log \pi_\theta(a_t|s_t)
$$
Let’s think about what happens at time-step *t*. Even before an action is decided, the agent has already received all the rewards up until step t-1*t*−1. So we can think of that part of the total reward as the reward from the past. The rest is denoted as the future reward.
$$
(\overbrace{...+r_{t-1}}^{\cancel{R^{\rm past}_t}}+ \overbrace{r_{t}+...}^{R^{\rm future}_t})
$$
Because we have a Markov process, the action at time-step *t* can only affect the future reward, so the past reward shouldn’t be contributing to the policy gradient. So to properly assign credit to the action $a_t$, we should ignore the past reward. So a better policy gradient would simply have the future reward as the coefficient .
$$
g=\sum_t R_t^{\rm future}\nabla_{\theta}\log \pi_\theta(a_t|s_t)
$$


**Notes on Gradient Modification**

You might wonder, why is it okay to just change our gradient? Wouldn't that change our original goal of maximizing the expected reward?

It turns out that mathematically, ignoring past rewards might change the gradient for each specific trajectory, but it doesn't change the **averaged** gradient. So even though the gradient is different during training, on average we are still maximizing the average reward. In fact, the resultant gradient is less noisy, so training using future reward should speed things up!



## Policy Gradient Quiz

Suppose we are training an agent to play a computer game. There are only two possible action:

0 = Do nothing, 1 = Move

There are three time-steps in each game, and our policy is completely determined by one parameter \theta*θ*, such that the probability of "moving" is $\theta$, and the probability of doing nothing is $1-\theta$

Initially $\theta=0.5$. Three games are played, the results are:

Game 1: actions: (1,0,1) rewards: (1,0,1)
Game 2: actions: (1,0,0) rewards: (0,0,1)
Game 3: actions: (0,1,0) rewards: (1,0,1)



**Question 1:** What are the future rewards for the first game?
Recall the results for game 1 are: actions: (1,0,1) rewards: (1,0,1)

Answer to Q1: (2,1,1)

**Question 2:** What is the policy gradient computed from the second game, using future rewards?
actions: (1,0,0) rewards: (0,0,1)

Make sure you are using the fact that $\pi_\theta(1|s_t) = \theta$, and $\pi_{\theta}(0|s_t) = 1-\theta$.  The future rewards are computed to be (1,1,1). 
$$
\sum_t \nabla_{\theta} \log\pi_{\theta}(a_t | s_t) R_{t}^{\rm future} = \sum_t \frac{\nabla_{\theta} \pi_{\theta}(a_t | s_t)}{\pi_{\theta}(a_t | s_t)} R_{t}^{\rm future}=
$$

$$
\frac{\nabla_{\theta} \pi_{\theta}(a_0 | s_0)}{\pi_{\theta}(a_0 | s_0)} R_{0}^{future}+
\frac{\nabla_{\theta} \pi_{\theta}(a_1 | s_1)}{\pi_{\theta}(a_1 | s_1)} R_{1}^{future}+
\frac{\nabla_{\theta} \pi_{\theta}(a_2 | s_2)}{\pi_{\theta}(a_2 | s_2)} R_{2}^{future}=
$$

$$
\frac{\nabla_{\theta} \theta}{\theta} 1+
\frac{\nabla_{\theta} (1-\theta)}{1-\theta} 1+
\frac{\nabla_{\theta} (1-\theta)}{1-\theta} 1=\\
\frac{1}{0.5} 1+
\frac{-1}{0.5} 1+
\frac{-1}{0.5} 1=-2
$$

Each time an action 1 is taken, it contributes $+1/0.5 = +2$ to the policy gradient. Whereas every time an action 0 is taken, it contributes $-1/0.5 = -2$ to the gradient. This is be because $\pi_{\theta}(0|s)= 1-\theta$

**Question 3:** In game 3 (reminder - actions: (0,1,0) rewards: (1,0,1)) the future rewards are computed to be (2,1,1)
$$
\frac{\nabla_{\theta} (1-\theta)}{1-\theta} 2+
\frac{\nabla_{\theta} \theta}{\theta} 1+
\frac{\nabla_{\theta} (1-\theta)}{1-\theta} 1=\\
\frac{-1}{0.5} 2+\frac{1}{0.5} 1+\frac{-1}{0.5} 1=-2
$$
Therefore, the following is correct:

- The contribution to the gradient from the second and third steps cancel each other
- The computed policy gradient from this game is negative
- Using the total reward vs future reward give the same policy gradient in this game



## Coding exercise - Pong with REINFORCE

