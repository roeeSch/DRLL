# Policy Gradient Methods

Policy gradient methods are a subclass of policy-based methods. In this lesson, we'll confine our attention to stochastic policies.

Policy gradient intuition:

For successful episodes go over each state-action in the episode and change the network weights to slightly increase the probabilities of choosing this action in this state. Do the opposite for unsuccessful episodes.

<img src='images\pgm-big-picture.png' style='zoom:60%'>



## Connections to Supervised Learning

Policy gradient methods are very similar to supervised learning.

<img src='images\pgm-similarity-to-supLearning.png' style='zoom:60%'>

The difference is that in supervised learning the dataset doesn't change and in policy gradient methods we use an episode, update the weights for each state-action pair and discard the dataset and continue on to collect another episode \ dataset.

Unlike image classification where an image of a dog wont appear in both classes in the dataset, in policy gradient methods it could happen. This increases the difficulty of our  task.

### Learn More

To further explore the connections between policy gradient methods and supervised learning, you're encouraged to check out Andrej Karpathy's [famous blog post](http://karpathy.github.io/2016/05/31/rl/).

<img src='images\pgm-similarity-to-supLearning2.png' style='zoom:30%'>





## Problem Setup

Definitions (H- horizon, trajectory, Reward):

<img src='images\pgm-definistion.png' style='zoom:60%'>

Goal:

<img src='images\pgm-definistion-goal.png' style='zoom:60%'>

The semicolon is to indicate that $\theta$ has an influence on the probability of the trajectory.

**<u>Note:</u>** $U(\theta)$ is the **expectation** of the return.



## Why Trajectories?

You may be wondering: *why are we using trajectories instead of episodes?* The answer is that maximizing expected return over trajectories (instead of episodes) lets us search for optimal policies for both episodic *and continuing* tasks!

That said, for many episodic tasks, it often makes sense to just use the full episode. In particular, for the case of the video game example described in the lessons, reward is only delivered at the end of the episode. In this case, in order to estimate the expected return, the trajectory should correspond to the full episode; otherwise, we don't have enough reward information to meaningfully estimate the expected return.

## REINFORCE

You've learned that our goal is to find the values of the weights $\theta$ in the neural network that maximize the expected return U:

​						$U(\theta)=\sum_{\tau}{P(\tau;\theta)R(\tau)}$

where $\tau$ is an arbitrary trajectory. One way to determine the value of $\theta$ that maximizes this function is through **gradient ascent**. This algorithm is closely related to **gradient descent**, where the differences are that:

- gradient descent is designed to find the **minimum** of a function, whereas gradient ascent will find the **maximum**, and
- gradient descent steps in the direction of the **negative gradient**, whereas gradient ascent steps in the direction of the **gradient**.

Our update step for gradient ascent appears as follows:

$\theta \leftarrow \theta + \alpha \nabla_\theta U(\theta)$

where $\alpha$ is the step size that is generally allowed to decay over time. Once we know how to calculate or estimate this gradient, we can repeatedly apply this update step, in the hopes that $\theta$ converges to the value that maximizes $U(\theta)$.



### Pseudo Code:

Since calculating the gradient is not pragmatic we will estimate the gradient by in several (few) trajectories.

The algorithm described in the video is known as **REINFORCE**. The pseudocode is summarized below.

1. Use the policy $\pi_\theta$ to collect *m* trajectories $\{ \tau^{(1)}, \tau^{(2)}, \ldots, \tau^{(m)}\}$ with horizon *H*. We refer to the *i*-th trajectory as

   $\tau^{(i)} = (s_0^{(i)}, a_0^{(i)}, \ldots, s_H^{(i)}, a_H^{(i)}, s_{H+1}^{(i)})$.

2. Use the trajectories to estimate the gradient $\nabla_\theta U(\theta)$:

   $\nabla_\theta U(\theta) \approx \hat{g} := \frac{1}{m}\sum_{i=1}^m \sum_{t=0}^{H} \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)}) R(\tau^{(i)})$

   $\log \pi_\theta(a_t^{(i)}|s_t^{(i)})$ - the log probability of selecting action $a_{t}^{(i)}$ at state $s_{t}^{(i)}$ 

3. Update the weights of the policy:

   $\theta \leftarrow \theta + \alpha \hat{g}$

4. Loop over steps 1-3.



## Derivation of (Optional) 

How to derive the equation that approximates the gradient?

### Likelihood Ratio Policy Gradient

------

We'll begin by exploring how to calculate the gradient $\nabla_\theta U(\theta)$. The calculation proceeds as follows:

$$
\begin{aligned}\nabla_\theta U(\theta) &= \nabla_\theta \sum_\tau P(\tau;\theta)R(\tau) & (1)\\ &= \sum_\tau \nabla_\theta P(\tau;\theta)R(\tau) & (2)\\ &= \sum_\tau \frac{P(\tau;\theta)}{P(\tau;\theta)} \nabla_\theta P(\tau;\theta)R(\tau) & (3)\\ &= \sum_\tau P(\tau;\theta) \frac{\nabla_\theta P(\tau;\theta)}{P(\tau;\theta)}R(\tau) & (4)\\ &= \sum_\tau P(\tau;\theta) \nabla_\theta \log P(\tau;\theta) R(\tau) & (5) \end{aligned}
$$

First, we note line (1) follows directly from $U(\theta) = \sum_\tau P(\tau;\theta)R(\tau)$, where we've only taken the gradient of both sides.

Then, we can get line (2) by just noticing that we can rewrite the gradient of the sum as the sum of the gradients.

In line (3), we only multiply every term in the sum by $\frac{P(\tau;\theta)}{P(\tau;\theta)}$, which is perfectly allowed because this fraction is equal to one!

Next, line (4) is just a simple rearrangement of the terms from the previous line. That is, $\frac{P(\tau;\theta)}{P(\tau;\theta)} \nabla_\theta P(\tau;\theta) = P(\tau;\theta) \frac{\nabla_\theta P(\tau;\theta)}{P(\tau;\theta)}$.

Finally, line (5) follows from the chain rule, and the fact that the gradient of the log of a function is always equal to the gradient of the function, divided by the function. (*In case it helps to see this with simpler notation, recall that $\nabla_x \log f(x) = \frac{\nabla_x f(x)}{f(x)}$) Thus, $\nabla_\theta \log P(\tau;\theta) = \frac{\nabla_\theta P(\tau;\theta)}{P(\tau;\theta)}$.

The final "trick" that yields line (5) (i.e., $\nabla_\theta \log P(\tau;\theta) = \frac{\nabla_\theta P(\tau;\theta)}{P(\tau;\theta)}$) is referred to as the **likelihood ratio trick** or **REINFORCE trick**.

Likewise, it is common to refer to the gradient as the **likelihood ratio policy gradient**: 


$$
\nabla_\theta U(\theta) = \sum_\tau P(\tau;\theta) \nabla_\theta \log P(\tau;\theta) R(\tau)
$$

Once we’ve written the gradient as an expected value in this way, it becomes much easier to estimate.



### Sample-Based Estimate

We can approximate the likelihood ratio policy gradient with a sample-based average, as shown below:

$$
\nabla_\theta U(\theta) \approx \frac{1}{m}\sum_{i=1}^m \nabla_\theta \log \mathbb{P}(\tau^{(i)};\theta)R(\tau^{(i)})
$$

where each $\tau^{(i)}$ is a sampled trajectory.



### Finishing the Calculation

Before calculating the expression above, we will need to further simplify $\nabla_\theta \log \mathbb{P}(\tau^{(i)};\theta)$. The derivation proceeds as follows:

$$
\begin{aligned} \nabla_\theta \log \mathbb{P}(\tau^{(i)};\theta) &= \nabla_\theta \log \Bigg[ \prod_{t=0}^{H} \mathbb{P}(s_{t+1}^{(i)}|s_{t}^{(i)}, a_t^{(i)} )\pi_\theta(a_t^{(i)}|s_t^{(i)}) \Bigg] & (1)\\ &= \nabla_\theta \Bigg[ \sum_{t=0}^{H} \log \mathbb{P}(s_{t+1}^{(i)}|s_{t}^{(i)}, a_t^{(i)} ) + \sum_{t=0}^{H}\log \pi_\theta(a_t^{(i)}|s_t^{(i)}) \Bigg] & (2)\\ &= \nabla_\theta\sum_{t=0}^{H} \log \mathbb{P}(s_{t+1}^{(i)}|s_{t}^{(i)}, a_t^{(i)} ) + \nabla_\theta \sum_{t=0}^{H}\log \pi_\theta(a_t^{(i)}|s_t^{(i)}) & (3)\\ &= \nabla_\theta \sum_{t=0}^{H}\log \pi_\theta(a_t^{(i)}|s_t^{(i)}) & (4)\\ &= \sum_{t=0}^{H} \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)}) & (5) \end{aligned}
$$

First, line (1) shows how to calculate the probability of an arbitrary trajectory $\tau^{(i)}$. Namely, 
$\mathbb{P}(\tau^{(i)};\theta) = \prod_{t=0}^{H} \mathbb{P}(s_{t+1}^{(i)}|s_{t}^{(i)}, a_t^{(i)} )\pi_\theta(a_t^{(i)}|s_t^{(i)})$, where we have to take into account the action-selection probabilities from the policy and the state transition dynamics of the MDP.

Then, line (2) follows from the fact that the log of a product is equal to the sum of the logs.

Then, line (3) follows because the gradient of the sum can be written as the sum of gradients.

Next, line (4) holds, because $\sum_{t=0}^{H} \log \mathbb{P}(s_{t+1}^{(i)}|s_{t}^{(i)}, a_t^{(i)} )$ has no dependence on $\theta$, so $\nabla_\theta\sum_{t=0}^{H} \log \mathbb{P}(s_{t+1}^{(i)}|s_{t}^{(i)}, a_t^{(i)} )=0$.

Finally, line (5) holds, because we can rewrite the gradient of the sum as the sum of gradients.

**That's it!!!**

Plugging in the calculation above yields the equation for estimating the gradient:

$$
\nabla_\theta U(\theta) \approx \hat{g} = \frac{1}{m}\sum_{i=1}^m \sum_{t=0}^{H} \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)}) R(\tau^{(i)})
$$


# Coding exercise - REINFORCE

solve CartPole-v0

# What's Next?



In this lesson, you've learned all about the REINFORCE algorithm, which was illustrated with a toy environment with a **discrete** action space. But it's also important to mention that REINFORCE can also be used to solve environments with continuous action spaces!

For an environment with a continuous action space, the corresponding policy network could have an output layer that parametrizes a [continuous probability distribution](https://en.wikipedia.org/wiki/Probability_distribution#Continuous_probability_distribution).

For instance, assume the output layer returns the mean \mu*μ* and variance \sigma^2*σ*2 of a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution).



<img src='images\350px-normal-distribution-pdf.png'>

Probability density function corresponding to normal distribution (Source: Wikipedia)



Then in order to select an action, the agent needs only to pass the most recent state s_t*s**t* as input to the network, and then use the output mean \mu*μ* and variance \sigma^2*σ*2 to sample from the distribution a_t\sim\mathcal{N}(\mu, \sigma^2)*a**t*∼N(*μ*,*σ*2).

This should work in theory, but it's unlikely to perform well in practice! To improve performance with continuous action spaces, we'll have to make some small modifications to the REINFORCE algorithm, and you'll learn more about these modifications in the upcoming lessons.



## Summary

<img src='images\reinforce_space.png'>

REINFORCE increases the probability of "good" actions and decreases the probability of "bad" actions ([Source](https://blog.openai.com/evolution-strategies/))



### What are Policy Gradient Methods?

------

- **Policy-based methods** are a class of algorithms that search directly for the optimal policy, without simultaneously maintaining value function estimates.
- **Policy gradient methods** are a subclass of policy-based methods that estimate the weights of an optimal policy through gradient ascent.
- In this lesson, we represent the policy with a neural network, where our goal is to find the weights $\theta$ of the network that maximize expected return.

### The Big Picture

------

- The policy gradient method will iteratively amend the policy network weights to:
  - make (state, action) pairs that resulted in positive return more likely, and
  - make (state, action) pairs that resulted in negative return less likely.

### Problem Setup

- A **trajectory** $\tau$ is a state-action sequence $s_0, a_0, \ldots, s_H, a_H, s_{H+1}$.
- In this lesson, we will use the notation $R(\tau)$ to refer to the return corresponding to trajectory $\tau$.
- Our goal is to find the weights $\theta$ of the policy network to maximize the **expected return** $U(\theta) := \sum_\tau \mathbb{P}(\tau;\theta)R(\tau)$.

### REINFORCE

- The pseudocode for REINFORCE is as follows:
  1. Use the policy $\pi_\theta$ to collect *m* trajectories $\{ \tau^{(1)}, \tau^{(2)}, \ldots, \tau^{(m)}\}$ with horizon *H*. We refer to the *i*-th trajectory as 
$$
\tau^{(i)} = (s_0^{(i)}, a_0^{(i)}, \ldots, s_H^{(i)}, a_H^{(i)}, s_{H+1}^{(i)})
$$
  2. Use the trajectories to estimate the gradient $\nabla_\theta U(\theta)$:

$$
\nabla_\theta U(\theta) \approx \hat{g} := \frac{1}{m}\sum_{i=1}^m \sum_{t=0}^{H} \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)}) R(\tau^{(i)})
$$

  3. Update the weights of the policy:

$$
\theta \leftarrow \theta + \alpha \hat{g}
$$

  4. Loop over steps 1-3.

### Derivation

------

- We derived the **likelihood ratio policy gradient**: $\nabla_\theta U(\theta) = \sum_\tau \mathbb{P}(\tau;\theta)\nabla_\theta \log \mathbb{P}(\tau;\theta)$.
- We can approximate the gradient above with a sample-weighted average:
$$
\nabla_\theta U(\theta) \approx \frac{1}{m}\sum_{i=1}^m \nabla_\theta \log \mathbb{P}(\tau^{(i)};\theta)R(\tau^{(i)})
$$
- We calculated the following:
$$
\nabla_\theta \log \mathbb{P}(\tau^{(i)};\theta) = \sum_{t=0}^{H} \nabla_\theta \log \pi_\theta (a_t^{(i)}|s_t^{(i)})
$$

### What's Next?

------

- REINFORCE can solve Markov Decision Processes (MDPs) with either discrete or continuous action spaces.